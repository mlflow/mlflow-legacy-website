"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3975],{66860:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"tracing/production","title":"Tracing in Production","description":"Machine learning projects don\'t conclude with their initial launch. Ongoing monitoring and incremental enhancements are critical for long-term success. MLflow Tracing offers observability for your production application, supporting the iterative process of continuous improvement.","source":"@site/docs/tracing/production.mdx","sourceDirName":"tracing","slug":"/tracing/production","permalink":"/docs/2.20.1.dev0/tracing/production","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"sidebar_label":"Production Monitoring"},"sidebar":"docsSidebar","previous":{"title":"Low-level Client APIs","permalink":"/docs/2.20.1.dev0/tracing/api/client"},"next":{"title":"Session","permalink":"/docs/2.20.1.dev0/tracing/session"}}');var r=t(74848),i=t(28453);const s={sidebar_position:6,sidebar_label:"Production Monitoring"},a="Tracing in Production",c={},l=[{value:"Self-host Tracking Server",id:"self-host-tracking-server",level:2},{value:"OpenTelemetry Collector",id:"opentelemetry-collector",level:2},{value:"Configurations",id:"configurations",level:3},{value:"Databricks Managed MLflow",id:"databricks-managed-mlflow",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"tracing-in-production",children:"Tracing in Production"})}),"\n",(0,r.jsxs)(n.p,{children:["Machine learning projects don't conclude with their initial launch. Ongoing monitoring and incremental enhancements are critical for long-term success. ",(0,r.jsx)(n.a,{href:"/tracing",children:"MLflow Tracing"})," offers observability for your production application, supporting the iterative process of continuous improvement."]}),"\n",(0,r.jsxs)(n.p,{children:["When it comes to production monitoring, there are many additional considerations for tracing, such as scalability, security, and cost. Moreover, ",(0,r.jsx)(n.strong,{children:"monitoring the machine learning model alone never be sufficient in the real software operation"}),". Engineering teams need to monitor multiple services throughout the entire product and ensure that it delivers expected values to end users. Thereby, MLflow supports integration with various observability solutions such as Grafana, Datadog, New Relic, and more, with the power of OpenTelemetry standardization."]}),"\n",(0,r.jsx)(n.h2,{id:"self-host-tracking-server",children:"Self-host Tracking Server"}),"\n",(0,r.jsx)(n.p,{children:"Of course, you can keep using the MLflow tracking server to store production traces. However, tracking server is optimized for offline experience and generally not suitable for handling the hyper scale traffic. Thereby, we recommend using the other two options for production monitoring use case."}),"\n",(0,r.jsxs)(n.p,{children:["If you choose to keep using the tracking server in production, we ",(0,r.jsx)(n.strong,{children:"strongly recommend"})," using SQL-based tracking server on top of a scalable database and artifact storage, as it will be a key factor for write and query performance. Refer to the ",(0,r.jsx)(n.a,{href:"/tracking#tracking-setup",children:"tracking server setup guide"})," for more details. In addition, tracking server by default uses infinite retention date for trace data, hence it is recommended to set up periodical deletion job using the SDK or REST API."]}),"\n",(0,r.jsx)(n.h2,{id:"opentelemetry-collector",children:"OpenTelemetry Collector"}),"\n",(0,r.jsxs)(n.p,{children:["Traces generated by MLflow are compatible with the ",(0,r.jsx)(n.a,{href:"https://opentelemetry.io/docs/specs/otel/trace/api/#span",children:"OpenTelemetry trace specs"}),".\nTherefore, MLflow traces can be exported to various observability solutions that support OpenTelemetry."]}),"\n",(0,r.jsx)(n.p,{children:"// TODO: Add list of icons"}),"\n",(0,r.jsxs)(n.p,{children:["By default, MLflow exports traces to the MLflow Tracking Server. To enable exporting traces to an OpenTelemetry Collector, set the ",(0,r.jsx)(n.code,{children:"OTEL_EXPORTER_OTLP_ENDPOINT"})," environment variable (or ",(0,r.jsx)(n.code,{children:"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"}),") to the target URL of the OpenTelemetry Collector ",(0,r.jsx)(n.strong,{children:"before starting any trace"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import mlflow\nimport os\n\n# Set the endpoint of the OpenTelemetry Collector\nos.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = "http://localhost:4317/v1/traces"\n# Optionally, set the service name to group traces\nos.environ["OTEL_SERVICE_NAME"] = "<your-service-name>"\n\n# Trace will be exported to the OTel collector at http://localhost:4317/v1/traces\nwith mlflow.start_span(name="foo") as span:\n    span.set_inputs({"a": 1})\n    span.set_outputs({"b": 2})\n'})}),"\n",(0,r.jsxs)(n.admonition,{type:"warning",children:[(0,r.jsxs)(n.p,{children:["MLflow only exports traces to a single destination. When the ",(0,r.jsx)(n.code,{children:"OTEL_EXPORTER_OTLP_ENDPOINT"})," environment variable is configured, MLflow will ",(0,r.jsx)(n.strong,{children:"not"})," export traces to the MLflow Tracking Server and you will not see traces in the MLflow UI."]}),(0,r.jsxs)(n.p,{children:["Similarly, if you deploy the model to the ",(0,r.jsx)(n.a,{href:"https://docs.databricks.com/en/mlflow/mlflow-tracing.html#use-mlflow-tracing-in-production",children:"Databricks Model Serving with tracing enabled"}),", using the OpenTelemetry Collector will result in traces not being recorded in the Inference Table."]})]}),"\n",(0,r.jsx)(n.h3,{id:"configurations",children:"Configurations"}),"\n",(0,r.jsxs)(n.p,{children:["MLflow uses the standard OTLP Exporter for exporting traces to OpenTelemetry Collector instances. Thereby, you can use ",(0,r.jsx)(n.a,{href:"https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/",children:"all of the configurations"})," supported by OpenTelemetry. The following example configures the OTLP Exporter to use HTTP protocol instead of the default gRPC and sets custom headers:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="http://localhost:4317/v1/traces"\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL="http/protobuf"\nexport OTEL_EXPORTER_OTLP_TRACES_HEADERS="api_key=12345"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"databricks-managed-mlflow",children:"Databricks Managed MLflow"}),"\n",(0,r.jsxs)(n.p,{children:["If you are using ",(0,r.jsx)(n.a,{href:"https://www.databricks.com/product/managed-mlflow",children:"Databricks Managed MLflow"})," for building your machine learning models, the set up for production tracing is easy. When deploying a model to a ",(0,r.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/index.html",children:"Databricks Model Serving"})," endpoint:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["check the ",(0,r.jsx)(n.strong,{children:'"Enable Tracing"'})," checkbox or manually specify ",(0,r.jsx)(n.code,{children:"ENABLE_MLFLOW_TRACING"})," environment to ",(0,r.jsx)(n.code,{children:"True"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Enable ",(0,r.jsx)(n.a,{href:"https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html",children:"inference table"})," for the serving endpoint."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"By doing so, MLflow exports the traces to the table for the serving endpoint along with the request and response logs."}),"\n",(0,r.jsxs)(n.p,{children:["Since the inference table is a Delta Table governed by Unity Catalog, it offers extremely high scalability and fine-grained access control, making it an ideal place to store your trace data securely. Refer to the ",(0,r.jsx)(n.a,{href:"https://docs.databricks.com/en/mlflow/mlflow-tracing.html#use-mlflow-tracing-in-production",children:"Databricks MLflow Tracing documentation"})," for more details about this setup."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(96540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);