

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/pyfunc/model -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.pyfunc.model</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/pyfunc/model.html">
  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    

    

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-N6WMTTJ");</script>
        <!-- End Google Tag Manager -->
    
  
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=AW-16857946923"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'AW-16857946923');
  </script>
  <!-- Eng gtag -->

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="MLflow 2.20.3 documentation" href="../../../index.html"/>
        <link rel="up" title="mlflow.pyfunc" href="../pyfunc.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../_static/tabs.js"></script>
<script type="text/javascript" src="../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script type="text/javascript" src="../../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N6WMTTJ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../../../index.html" class="wy-nav-top-logo"
      ><img src="../../../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.20.3</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../index.html" class="main-navigation-home"><img src="../../../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/index.html">MLflow Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../new-features/index.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../llms/index.html">LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../llms/tracing/index.html">MLflow Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../pyfunc.html">mlflow.pyfunc</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.pyfunc.model</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/pyfunc/model" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.pyfunc.model</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The ``mlflow.pyfunc.model`` module defines logic for saving and loading custom &quot;python_function&quot;</span>
<span class="sd">models with a user-defined ``PythonModel`` subclass.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">cloudpickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">yaml</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">mlflow.pyfunc</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mlflow.utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.exceptions</span><span class="w"> </span><span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.models.model</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">,</span> <span class="n">MODEL_CODE_PATH</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.models.rag_signatures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatCompletionRequest</span><span class="p">,</span> <span class="n">SplitChatMessagesRequest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.models.signature</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_extract_type_hints</span><span class="p">,</span>
    <span class="n">_is_context_in_predict_function_signature</span><span class="p">,</span>
    <span class="n">_TypeHints</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.models.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_load_model_code_path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.protos.databricks_pb2</span><span class="w"> </span><span class="kn">import</span> <span class="n">INVALID_PARAMETER_VALUE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.pyfunc.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyfunc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.pyfunc.utils.data_validation</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_check_func_signature</span><span class="p">,</span>
    <span class="n">_get_func_info_if_type_hint_supported</span><span class="p">,</span>
    <span class="n">_wrap_chat_agent_predict</span><span class="p">,</span>
    <span class="n">_wrap_predict_with_pyfunc</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.pyfunc.utils.input_converter</span><span class="w"> </span><span class="kn">import</span> <span class="n">_hydrate_dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.tracking.artifact_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_download_artifact_from_uri</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.types.agent</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ChatAgentChunk</span><span class="p">,</span>
    <span class="n">ChatAgentMessage</span><span class="p">,</span>
    <span class="n">ChatAgentResponse</span><span class="p">,</span>
    <span class="n">ChatContext</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.types.llm</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ChatCompletionChunk</span><span class="p">,</span>
    <span class="n">ChatCompletionResponse</span><span class="p">,</span>
    <span class="n">ChatMessage</span><span class="p">,</span>
    <span class="n">ChatParams</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.types.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_is_list_dict_str</span><span class="p">,</span> <span class="n">_is_list_str</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.annotations</span><span class="w"> </span><span class="kn">import</span> <span class="n">deprecated</span><span class="p">,</span> <span class="n">experimental</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.databricks_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_databricks_serverless_env_vars</span><span class="p">,</span>
    <span class="n">is_in_databricks_serverless_runtime</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.environment</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_mlflow_conda_env</span><span class="p">,</span>
    <span class="n">_process_conda_env</span><span class="p">,</span>
    <span class="n">_process_pip_requirements</span><span class="p">,</span>
    <span class="n">_PythonEnv</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.file_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">TempDir</span><span class="p">,</span> <span class="n">get_total_file_size</span><span class="p">,</span> <span class="n">write_to</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.model_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_flavor_configuration</span><span class="p">,</span> <span class="n">_validate_infer_and_copy_code_paths</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.requirements_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_pinned_requirement</span>

<span class="n">CONFIG_KEY_ARTIFACTS</span> <span class="o">=</span> <span class="s2">&quot;artifacts&quot;</span>
<span class="n">CONFIG_KEY_ARTIFACT_RELATIVE_PATH</span> <span class="o">=</span> <span class="s2">&quot;path&quot;</span>
<span class="n">CONFIG_KEY_ARTIFACT_URI</span> <span class="o">=</span> <span class="s2">&quot;uri&quot;</span>
<span class="n">CONFIG_KEY_PYTHON_MODEL</span> <span class="o">=</span> <span class="s2">&quot;python_model&quot;</span>
<span class="n">CONFIG_KEY_CLOUDPICKLE_VERSION</span> <span class="o">=</span> <span class="s2">&quot;cloudpickle_version&quot;</span>
<span class="n">_SAVED_PYTHON_MODEL_SUBPATH</span> <span class="o">=</span> <span class="s2">&quot;python_model.pkl&quot;</span>
<span class="n">_DEFAULT_CHAT_MODEL_METADATA_TASK</span> <span class="o">=</span> <span class="s2">&quot;agent/v1/chat&quot;</span>
<span class="n">_DEFAULT_CHAT_AGENT_METADATA_TASK</span> <span class="o">=</span> <span class="s2">&quot;agent/v2/chat&quot;</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="get_default_pip_requirements"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.get_default_pip_requirements">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_default_pip_requirements</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        A list of default pip requirements for MLflow Models produced by this flavor. Calls to</span>
<span class="sd">        :func:`save_model()` and :func:`log_model()` produce a pip environment that, at minimum,</span>
<span class="sd">        contains these requirements.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_get_pinned_requirement</span><span class="p">(</span><span class="s2">&quot;cloudpickle&quot;</span><span class="p">)]</span></div>


<div class="viewcode-block" id="get_default_conda_env"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.get_default_conda_env">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_default_conda_env</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        The default Conda environment for MLflow Models produced by calls to</span>
<span class="sd">        :func:`save_model() &lt;mlflow.pyfunc.save_model&gt;`</span>
<span class="sd">        and :func:`log_model() &lt;mlflow.pyfunc.log_model&gt;` when a user-defined subclass of</span>
<span class="sd">        :class:`PythonModel` is provided.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_mlflow_conda_env</span><span class="p">(</span><span class="n">additional_pip_deps</span><span class="o">=</span><span class="n">get_default_pip_requirements</span><span class="p">())</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_log_warning_if_params_not_in_predict_signature</span><span class="p">(</span><span class="n">logger</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;The underlying model does not support passing additional parameters to the predict&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; function. `params` </span><span class="si">{</span><span class="n">params</span><span class="si">}</span><span class="s2"> will be ignored.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="PythonModel"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">PythonModel</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents a generic Python model that evaluates inputs and produces API-compatible outputs.</span>
<span class="sd">    By subclassing :class:`~PythonModel`, users can create customized MLflow models with the</span>
<span class="sd">    &quot;python_function&quot; (&quot;pyfunc&quot;) flavor, leveraging custom inference logic and artifact</span>
<span class="sd">    dependencies.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__metaclass__</span> <span class="o">=</span> <span class="n">ABCMeta</span>

<div class="viewcode-block" id="PythonModel.load_context"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel.load_context">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">load_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads artifacts from the specified :class:`~PythonModelContext` that can be used by</span>
<span class="sd">        :func:`~PythonModel.predict` when evaluating inputs. When loading an MLflow model with</span>
<span class="sd">        :func:`~load_model`, this method is called as soon as the :class:`~PythonModel` is</span>
<span class="sd">        constructed.</span>

<span class="sd">        The same :class:`~PythonModelContext` will also be available during calls to</span>
<span class="sd">        :func:`~PythonModel.predict`, but it may be more efficient to override this method</span>
<span class="sd">        and load artifacts from the context at model load time.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that the model</span>
<span class="sd">                     can use to perform inference.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;predict_type_hints&quot;</span><span class="p">,</span> <span class="s2">&quot;2.20.0&quot;</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_type_hints</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_type_hints</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predict_type_hints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_TypeHints</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal method to get type hints from the predict function signature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_predict_type_hints&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span>
        <span class="k">if</span> <span class="n">_is_context_in_predict_function_signature</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span> <span class="o">=</span> <span class="n">_extract_type_hints</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="n">input_arg_index</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span> <span class="o">=</span> <span class="n">_extract_type_hints</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="n">input_arg_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init_subclass__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># automatically wrap the predict method with pyfunc to ensure data validation</span>
        <span class="c1"># NB: skip wrapping for built-in classes defined in MLflow e.g. ChatModel</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mlflow.&quot;</span><span class="p">):</span>
            <span class="c1">#  TODO: ChatModel uses dataclass type hints which are not supported now, hence</span>
            <span class="c1">#    we need to skip type hint based validation for user-defined subclasses</span>
            <span class="c1">#    of ChatModel. Once we either (1) support dataclass type hints or (2) migrate</span>
            <span class="c1">#    ChatModel to pydantic, we can remove this exclusion.</span>
            <span class="c1">#    NB: issubclass(cls, ChatModel) does not work so we use a hacky attribute check</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_skip_type_hint_validation&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="k">return</span>

            <span class="n">predict_attr</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;predict&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">predict_attr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">predict_attr</span><span class="p">):</span>
                <span class="n">func_info</span> <span class="o">=</span> <span class="n">_get_func_info_if_type_hint_supported</span><span class="p">(</span><span class="n">predict_attr</span><span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span><span class="p">,</span> <span class="n">_wrap_predict_with_pyfunc</span><span class="p">(</span><span class="n">predict_attr</span><span class="p">,</span> <span class="n">func_info</span><span class="p">))</span>
            <span class="n">predict_stream_attr</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;predict_stream&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">predict_stream_attr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">predict_stream_attr</span><span class="p">):</span>
                <span class="n">_check_func_signature</span><span class="p">(</span><span class="n">predict_stream_attr</span><span class="p">,</span> <span class="s2">&quot;predict_stream&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="PythonModel.predict"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel.predict">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates a pyfunc-compatible input and produces a pyfunc-compatible output.</span>
<span class="sd">        For more information about the pyfunc input/output API, see the :ref:`pyfunc-inference-api`.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that the model</span>
<span class="sd">                     can use to perform inference.</span>
<span class="sd">            model_input: A pyfunc-compatible input for the model to evaluate.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        .. tip::</span>
<span class="sd">            Since MLflow 2.20.0, `context` parameter can be removed from `predict` function</span>
<span class="sd">            signature if it&#39;s not used. `def predict(self, model_input, params=None)` is valid.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="PythonModel.predict_stream"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel.predict_stream">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates a pyfunc-compatible input and produces an iterator of output.</span>
<span class="sd">        For more information about the pyfunc input API, see the :ref:`pyfunc-inference-api`.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that the model</span>
<span class="sd">                     can use to perform inference.</span>
<span class="sd">            model_input: A pyfunc-compatible input for the model to evaluate.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        .. tip::</span>
<span class="sd">            Since MLflow 2.20.0, `context` parameter can be removed from `predict_stream` function</span>
<span class="sd">            signature if it&#39;s not used.</span>
<span class="sd">            `def predict_stream(self, model_input, params=None)` is valid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<span class="k">class</span><span class="w"> </span><span class="nc">_FunctionPythonModel</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    When a user specifies a ``python_model`` argument that is a function, we wrap the function</span>
<span class="sd">    in an instance of this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>
        <span class="c1"># only wrap `func` if @pyfunc is not already applied</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s2">&quot;_is_pyfunc&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">pyfunc</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predict_type_hints</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_predict_type_hints&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span> <span class="o">=</span> <span class="n">_extract_type_hints</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">,</span> <span class="n">input_arg_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_type_hints</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_input</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            model_input: A pyfunc-compatible input for the model to evaluate.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># callable only supports one input argument for now</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>


<div class="viewcode-block" id="PythonModelContext"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModelContext">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">PythonModelContext</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A collection of artifacts that a :class:`~PythonModel` can use when performing inference.</span>
<span class="sd">    :class:`~PythonModelContext` objects are created *implicitly* by the</span>
<span class="sd">    :func:`save_model() &lt;mlflow.pyfunc.save_model&gt;` and</span>
<span class="sd">    :func:`log_model() &lt;mlflow.pyfunc.log_model&gt;` persistence methods, using the contents specified</span>
<span class="sd">    by the ``artifacts`` parameter of these methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">artifacts</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            artifacts: A dictionary of ``&lt;name, artifact_path&gt;`` entries, where ``artifact_path``</span>
<span class="sd">                is an absolute filesystem path to a given artifact.</span>
<span class="sd">            model_config: The model configuration to make available to the model at</span>
<span class="sd">                loading time.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_artifacts</span> <span class="o">=</span> <span class="n">artifacts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span> <span class="o">=</span> <span class="n">model_config</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">artifacts</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A dictionary containing ``&lt;name, artifact_path&gt;`` entries, where ``artifact_path`` is an</span>
<span class="sd">        absolute filesystem path to the artifact.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_artifacts</span>

    <span class="nd">@experimental</span>
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A dictionary containing ``&lt;config, value&gt;`` entries, where ``config`` is the name</span>
<span class="sd">        of the model configuration keys and ``value`` is the value of the given configuration.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span></div>


<div class="viewcode-block" id="ChatModel"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel">[docs]</a><span class="nd">@experimental</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ChatModel</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. tip::</span>
<span class="sd">        Since MLflow 2.20.2, we recommend using :py:class:`ChatAgent &lt;mlflow.pyfunc.ChatAgent&gt;`</span>
<span class="sd">        instead of :py:class:`ChatModel &lt;mlflow.pyfunc.ChatModel&gt;` unless you need strict</span>
<span class="sd">        compatibility with the OpenAI ChatCompletion API.</span>

<span class="sd">    A subclass of :class:`~PythonModel` that makes it more convenient to implement models</span>
<span class="sd">    that are compatible with popular LLM chat APIs. By subclassing :class:`~ChatModel`,</span>
<span class="sd">    users can create MLflow models with a ``predict()`` method that is more convenient</span>
<span class="sd">    for chat tasks than the generic :class:`~PythonModel` API. ChatModels automatically</span>
<span class="sd">    define input/output signatures and an input example, so manually specifying these values</span>
<span class="sd">    when calling :func:`mlflow.pyfunc.save_model() &lt;mlflow.pyfunc.save_model&gt;` is not necessary.</span>

<span class="sd">    See the documentation of the ``predict()`` method below for details on that parameters and</span>
<span class="sd">    outputs that are expected by the ``ChatModel`` API.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_skip_type_hint_validation</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="ChatModel.predict"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel.predict">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">],</span> <span class="n">params</span><span class="p">:</span> <span class="n">ChatParams</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatCompletionResponse</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates a chat input and produces a chat output.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that the model</span>
<span class="sd">                can use to perform inference.</span>
<span class="sd">            messages (List[:py:class:`ChatMessage &lt;mlflow.types.llm.ChatMessage&gt;`]):</span>
<span class="sd">                A list of :py:class:`ChatMessage &lt;mlflow.types.llm.ChatMessage&gt;`</span>
<span class="sd">                objects representing chat history.</span>
<span class="sd">            params (:py:class:`ChatParams &lt;mlflow.types.llm.ChatParams&gt;`):</span>
<span class="sd">                A :py:class:`ChatParams &lt;mlflow.types.llm.ChatParams&gt;` object</span>
<span class="sd">                containing various parameters used to modify model behavior during</span>
<span class="sd">                inference.</span>

<span class="sd">        .. tip::</span>
<span class="sd">            Since MLflow 2.20.0, `context` parameter can be removed from `predict` function</span>
<span class="sd">            signature if it&#39;s not used.</span>
<span class="sd">            `def predict(self, messages: list[ChatMessage], params: ChatParams)` is valid.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A :py:class:`ChatCompletionResponse &lt;mlflow.types.llm.ChatCompletionResponse&gt;`</span>
<span class="sd">            object containing the model&#39;s response(s), as well as other metadata.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="ChatModel.predict_stream"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel.predict_stream">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">],</span> <span class="n">params</span><span class="p">:</span> <span class="n">ChatParams</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">ChatCompletionChunk</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates a chat input and produces a chat output.</span>
<span class="sd">        Override this function to implement a real stream prediction.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that the model</span>
<span class="sd">                can use to perform inference.</span>
<span class="sd">            messages (List[:py:class:`ChatMessage &lt;mlflow.types.llm.ChatMessage&gt;`]):</span>
<span class="sd">                A list of :py:class:`ChatMessage &lt;mlflow.types.llm.ChatMessage&gt;`</span>
<span class="sd">                objects representing chat history.</span>
<span class="sd">            params (:py:class:`ChatParams &lt;mlflow.types.llm.ChatParams&gt;`):</span>
<span class="sd">                A :py:class:`ChatParams &lt;mlflow.types.llm.ChatParams&gt;` object</span>
<span class="sd">                containing various parameters used to modify model behavior during</span>
<span class="sd">                inference.</span>

<span class="sd">        .. tip::</span>
<span class="sd">            Since MLflow 2.20.0, `context` parameter can be removed from `predict_stream` function</span>
<span class="sd">            signature if it&#39;s not used.</span>
<span class="sd">            `def predict_stream(self, messages: list[ChatMessage], params: ChatParams)` is valid.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A generator over :py:class:`ChatCompletionChunk &lt;mlflow.types.llm.ChatCompletionChunk&gt;`</span>
<span class="sd">            object containing the model&#39;s response(s), as well as other metadata.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Streaming implementation not provided. Please override the &quot;</span>
            <span class="s2">&quot;`predict_stream` method on your model to generate streaming &quot;</span>
            <span class="s2">&quot;predictions&quot;</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="ChatAgent"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent">[docs]</a><span class="nd">@experimental</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ChatAgent</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **What is the ChatAgent Interface?**</span>

<span class="sd">    The ChatAgent interface is a chat schema specification that has been designed for authoring</span>
<span class="sd">    conversational agents. ChatAgent allows your agent to do the following:</span>

<span class="sd">    - Return multiple messages</span>
<span class="sd">    - Return intermediate steps for tool calling agents</span>
<span class="sd">    - Confirm tool calls</span>
<span class="sd">    - Support multi-agent scenarios</span>

<span class="sd">    ``ChatAgent`` should always be used when authoring an agent. We also recommend using</span>
<span class="sd">    ``ChatAgent`` instead of :py:class:`ChatModel &lt;mlflow.pyfunc.ChatModel&gt;` even for use cases</span>
<span class="sd">    like simple chat models (e.g. prompt-engineered LLMs), to give you the flexibility to support</span>
<span class="sd">    more agentic functionality in the future.</span>

<span class="sd">    The :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;` schema is similar to,</span>
<span class="sd">    but not strictly compatible with the OpenAI ChatCompletion schema. ChatAgent adds additional</span>
<span class="sd">    functionality and diverges from OpenAI</span>
<span class="sd">    :py:class:`ChatCompletionRequest &lt;mlflow.types.llm.ChatCompletionRequest&gt;` in the following</span>
<span class="sd">    ways:</span>

<span class="sd">    - Adds an optional ``attachments`` attribute to every input/output message for tools and</span>
<span class="sd">      internal agent calls so they can return additional outputs such as visualizations and progress</span>
<span class="sd">      indicators</span>
<span class="sd">    - Adds a ``context`` attribute with a ``conversation_id`` and ``user_id`` attributes to enable</span>
<span class="sd">      modifying the behavior of the agent depending on the user querying the agent</span>
<span class="sd">    - Adds the ``custom_inputs`` attribute, an arbitrary ``dict[str, Any]`` to pass in any</span>
<span class="sd">      additional information to modify the agent&#39;s behavior</span>

<span class="sd">    The :py:class:`ChatAgentResponse &lt;mlflow.types.agent.ChatAgentResponse&gt;` schema diverges from</span>
<span class="sd">    :py:class:`ChatCompletionResponse &lt;mlflow.types.llm.ChatCompletionResponse&gt;` schema in the</span>
<span class="sd">    following ways:</span>

<span class="sd">    - Adds the ``custom_outputs`` key, an arbitrary ``dict[str, Any]`` to return any additional</span>
<span class="sd">      information</span>
<span class="sd">    - Allows multiple messages in the output, to improve the  display and evaluation of internal</span>
<span class="sd">      tool calls and inter-agent communication that led to the final answer.</span>

<span class="sd">    Here&#39;s an example of a :py:class:`ChatAgentResponse &lt;mlflow.types.agent.ChatAgentResponse&gt;`</span>
<span class="sd">    detailing a tool call:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        {</span>
<span class="sd">            &quot;messages&quot;: [</span>
<span class="sd">                {</span>
<span class="sd">                    &quot;role&quot;: &quot;assistant&quot;,</span>
<span class="sd">                    &quot;content&quot;: &quot;&quot;,</span>
<span class="sd">                    &quot;id&quot;: &quot;run-04b46401-c569-4a4a-933e-62e38d8f9647-0&quot;,</span>
<span class="sd">                    &quot;tool_calls&quot;: [</span>
<span class="sd">                        {</span>
<span class="sd">                            &quot;id&quot;: &quot;call_15ca4fcc-ffa1-419a-8748-3bea34b9c043&quot;,</span>
<span class="sd">                            &quot;type&quot;: &quot;function&quot;,</span>
<span class="sd">                            &quot;function&quot;: {</span>
<span class="sd">                                &quot;name&quot;: &quot;generate_random_ints&quot;,</span>
<span class="sd">                                &quot;arguments&quot;: &#39;{&quot;min&quot;: 1, &quot;max&quot;: 100, &quot;size&quot;: 5}&#39;,</span>
<span class="sd">                            },</span>
<span class="sd">                        }</span>
<span class="sd">                    ],</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    &quot;role&quot;: &quot;tool&quot;,</span>
<span class="sd">                    &quot;content&quot;: &#39;{&quot;content&quot;: &quot;Generated array of 2 random ints in [1, 100].&quot;&#39;,</span>
<span class="sd">                    &quot;name&quot;: &quot;generate_random_ints&quot;,</span>
<span class="sd">                    &quot;id&quot;: &quot;call_15ca4fcc-ffa1-419a-8748-3bea34b9c043&quot;,</span>
<span class="sd">                    &quot;tool_call_id&quot;: &quot;call_15ca4fcc-ffa1-419a-8748-3bea34b9c043&quot;,</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    &quot;role&quot;: &quot;assistant&quot;,</span>
<span class="sd">                    &quot;content&quot;: &quot;The new set of generated random numbers are: 93, 51, 12, 7, and 25&quot;,</span>
<span class="sd">                    &quot;name&quot;: &quot;llm&quot;,</span>
<span class="sd">                    &quot;id&quot;: &quot;run-70c7c738-739f-4ecd-ad18-0ae232df24e8-0&quot;,</span>
<span class="sd">                },</span>
<span class="sd">            ],</span>
<span class="sd">            &quot;custom_outputs&quot;: {&quot;random_nums&quot;: [93, 51, 12, 7, 25]},</span>
<span class="sd">        }</span>

<span class="sd">    **Streaming Agent Output with ChatAgent**</span>

<span class="sd">    Please read the docstring of</span>
<span class="sd">    :py:func:`ChatAgent.predict_stream &lt;mlflow.pyfunc.ChatAgent.predict_stream&gt;`</span>
<span class="sd">    for more details on how to stream the output of your agent.</span>


<span class="sd">    **Authoring a ChatAgent**</span>

<span class="sd">    Authoring an agent using the ChatAgent  interface is a framework-agnostic way to create a model</span>
<span class="sd">    with a  standardized interface that is loggable with the MLflow pyfunc flavor, can be reused</span>
<span class="sd">    across clients, and is ready for serving workloads.</span>

<span class="sd">    To write your own agent, subclass ``ChatAgent``, implementing the ``predict`` and optionally</span>
<span class="sd">    ``predict_stream`` methods to define the non-streaming and streaming behavior of your agent. You</span>
<span class="sd">    can use any agent authoring framework - the only hard requirement is to implement the</span>
<span class="sd">    ``predict`` interface.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def predict(</span>
<span class="sd">            self,</span>
<span class="sd">            messages: list[ChatAgentMessage],</span>
<span class="sd">            context: Optional[ChatContext] = None,</span>
<span class="sd">            custom_inputs: Optional[dict[str, Any]] = None,</span>
<span class="sd">        ) -&gt; ChatAgentResponse: ...</span>

<span class="sd">    In addition to calling predict and predict_stream methods with an input matching their type</span>
<span class="sd">    hints, you can also pass a single input dict that matches the</span>
<span class="sd">    :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;` schema for ease of testing.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        chat_agent = MyChatAgent()</span>
<span class="sd">        chat_agent.predict(</span>
<span class="sd">            {</span>
<span class="sd">                &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 10 + 10?&quot;}],</span>
<span class="sd">                &quot;context&quot;: {&quot;conversation_id&quot;: &quot;123&quot;, &quot;user_id&quot;: &quot;456&quot;},</span>
<span class="sd">            }</span>
<span class="sd">        )</span>

<span class="sd">    See an example implementation of ``predict`` and ``predict_stream`` for a LangGraph agent in</span>
<span class="sd">    the :py:class:`ChatAgentState &lt;mlflow.langchain.chat_agent_langgraph.ChatAgentState&gt;`</span>
<span class="sd">    docstring.</span>

<span class="sd">    **Logging the ChatAgent**</span>

<span class="sd">    Since the landscape of LLM frameworks is constantly evolving and not every flavor can be</span>
<span class="sd">    natively supported by MLflow, we recommend the</span>
<span class="sd">    `Models-from-Code &lt;https://mlflow.org/docs/latest/model/models-from-code.html&gt;`_ logging</span>
<span class="sd">    approach.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        with mlflow.start_run():</span>
<span class="sd">            logged_agent_info = mlflow.pyfunc.log_model(</span>
<span class="sd">                artifact_path=&quot;agent&quot;,</span>
<span class="sd">                python_model=os.path.join(os.getcwd(), &quot;agent&quot;),</span>
<span class="sd">                # Add serving endpoints, tools, and vector search indexes here</span>
<span class="sd">                resources=[],</span>
<span class="sd">            )</span>

<span class="sd">    After logging the model, you can query the model with a single dictionary with the</span>
<span class="sd">    :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;` schema. Under the hood, it</span>
<span class="sd">    will be converted into the python objects expected by your ``predict`` and ``predict_stream``</span>
<span class="sd">    methods.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        loaded_model = mlflow.pyfunc.load_model(tmp_path)</span>
<span class="sd">        loaded_model.predict(</span>
<span class="sd">            {</span>
<span class="sd">                &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 10 + 10?&quot;}],</span>
<span class="sd">                &quot;context&quot;: {&quot;conversation_id&quot;: &quot;123&quot;, &quot;user_id&quot;: &quot;456&quot;},</span>
<span class="sd">            }</span>
<span class="sd">        )</span>

<span class="sd">    To make logging ChatAgent models as easy as possible, MLflow has built in the following</span>
<span class="sd">    features:</span>

<span class="sd">    - Automatic Model Signature Inference</span>
<span class="sd">        - You do not need to set a signature when logging a ChatAgent</span>
<span class="sd">        - An input and output signature will be automatically set that adheres to the</span>
<span class="sd">          :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;` and</span>
<span class="sd">          :py:class:`ChatAgentResponse &lt;mlflow.types.agent.ChatAgentResponse&gt;` schemas</span>
<span class="sd">    - Metadata</span>
<span class="sd">        - ``{“task”: “agent/v2/chat”}`` will be automatically appended to any metadata that you may</span>
<span class="sd">          pass in when logging the model</span>
<span class="sd">    - Input Example</span>
<span class="sd">        - Providng an input example is optional, ``mlflow.types.agent.CHAT_AGENT_INPUT_EXAMPLE``</span>
<span class="sd">          will be provided by default</span>
<span class="sd">        - If you do provide an input example, ensure it&#39;s a dict with the</span>
<span class="sd">          :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;` schema</span>

<span class="sd">        - .. code-block:: python</span>

<span class="sd">            input_example = {</span>
<span class="sd">                &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is MLflow?&quot;}],</span>
<span class="sd">                &quot;context&quot;: {&quot;conversation_id&quot;: &quot;123&quot;, &quot;user_id&quot;: &quot;456&quot;},</span>
<span class="sd">            }</span>

<span class="sd">    **Migrating from ChatModel to ChatAgent**</span>

<span class="sd">    To convert an existing ChatModel that takes in</span>
<span class="sd">    :py:class:`List[ChatMessage] &lt;mlflow.types.llm.ChatMessage&gt;` and</span>
<span class="sd">    :py:class:`ChatParams &lt;mlflow.types.llm.ChatParams&gt;` and outputs a</span>
<span class="sd">    :py:class:`ChatCompletionResponse &lt;mlflow.types.llm.ChatCompletionResponse&gt;`, do the following:</span>

<span class="sd">    - Subclass ``ChatAgent`` instead of ``ChatModel``</span>
<span class="sd">    - Move any functionality from your ``ChatModel``&#39;s ``load_context`` implementation into the</span>
<span class="sd">      ``__init__`` method of your new ``ChatAgent``.</span>
<span class="sd">    - Use ``.model_dump_compat()`` instead of ``.to_dict()`` when converting your model&#39;s inputs to</span>
<span class="sd">      dictionaries. Ex. ``[msg.model_dump_compat() for msg in messages]`` instead of</span>
<span class="sd">      ``[msg.to_dict() for msg in messages]``</span>
<span class="sd">    - Return a :py:class:`ChatAgentResponse &lt;mlflow.types.agent.ChatAgentResponse&gt;` instead of a</span>
<span class="sd">      :py:class:`ChatCompletionResponse &lt;mlflow.types.llm.ChatCompletionResponse&gt;`</span>

<span class="sd">    For example, we can convert the ChatModel from the</span>
<span class="sd">    `Chat Model Intro &lt;https://mlflow.org/docs/latest/llms/chat-model-intro/index.html#building-your-first-chatmodel&gt;`_</span>
<span class="sd">    to a ChatAgent:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        class SimpleOllamaModel(ChatModel):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                self.model_name = &quot;llama3.2:1b&quot;</span>
<span class="sd">                self.client = None</span>

<span class="sd">            def load_context(self, context):</span>
<span class="sd">                self.client = ollama.Client()</span>

<span class="sd">            def predict(</span>
<span class="sd">                self, context, messages: list[ChatMessage], params: ChatParams = None</span>
<span class="sd">            ) -&gt; ChatCompletionResponse:</span>
<span class="sd">                ollama_messages = [msg.to_dict() for msg in messages]</span>
<span class="sd">                response = self.client.chat(model=self.model_name, messages=ollama_messages)</span>
<span class="sd">                return ChatCompletionResponse(</span>
<span class="sd">                    choices=[{&quot;index&quot;: 0, &quot;message&quot;: response[&quot;message&quot;]}],</span>
<span class="sd">                    model=self.model_name,</span>
<span class="sd">                )</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        class SimpleOllamaModel(ChatAgent):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                self.model_name = &quot;llama3.2:1b&quot;</span>
<span class="sd">                self.client = None</span>
<span class="sd">                self.client = ollama.Client()</span>

<span class="sd">            def predict(</span>
<span class="sd">                self,</span>
<span class="sd">                messages: list[ChatAgentMessage],</span>
<span class="sd">                context: Optional[ChatContext] = None,</span>
<span class="sd">                custom_inputs: Optional[dict[str, Any]] = None,</span>
<span class="sd">            ) -&gt; ChatAgentResponse:</span>
<span class="sd">                ollama_messages = self._convert_messages_to_dict(messages)</span>
<span class="sd">                response = self.client.chat(model=self.model_name, messages=ollama_messages)</span>
<span class="sd">                return ChatAgentResponse(**{&quot;messages&quot;: [response[&quot;message&quot;]]})</span>

<span class="sd">    **ChatAgent Connectors**</span>

<span class="sd">    MLflow provides convenience APIs for wrapping agents written in popular authoring frameworks</span>
<span class="sd">    with ChatAgent. See examples for:</span>

<span class="sd">    - LangGraph in the</span>
<span class="sd">      :py:class:`ChatAgentState &lt;mlflow.langchain.chat_agent_langgraph.ChatAgentState&gt;` docstring</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_skip_type_hint_validation</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init_subclass__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span> <span class="s2">&quot;predict_stream&quot;</span><span class="p">):</span>
            <span class="n">attr</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">attr_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">attr</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">_wrap_chat_agent_predict</span><span class="p">(</span><span class="n">attr</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_convert_messages_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatAgentMessage</span><span class="p">]):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">model_dump_compat</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span>

    <span class="c1"># nb: We use `messages` instead of `model_input` so that the trace generated by default is</span>
    <span class="c1"># compatible with mlflow evaluate. We also want `custom_inputs` to be a top level key for</span>
    <span class="c1"># ease of use.</span>
<div class="viewcode-block" id="ChatAgent.predict"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent.predict">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatAgentMessage</span><span class="p">],</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ChatContext</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatAgentResponse</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a ChatAgent input, returns a ChatAgent output. In addition to calling ``predict``</span>
<span class="sd">        with an input matching the type hints, you can also pass a single input dict that matches</span>
<span class="sd">        the :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;` schema for ease</span>
<span class="sd">        of testing.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            chat_agent = ChatAgent()</span>
<span class="sd">            chat_agent.predict(</span>
<span class="sd">                {</span>
<span class="sd">                    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 10 + 10?&quot;}],</span>
<span class="sd">                    &quot;context&quot;: {&quot;conversation_id&quot;: &quot;123&quot;, &quot;user_id&quot;: &quot;456&quot;},</span>
<span class="sd">                }</span>
<span class="sd">            )</span>

<span class="sd">        Args:</span>
<span class="sd">            messages (List[:py:class:`ChatAgentMessage &lt;mlflow.types.agent.ChatAgentMessage&gt;`]):</span>
<span class="sd">                A list of :py:class:`ChatAgentMessage &lt;mlflow.types.agent.ChatAgentMessage&gt;`</span>
<span class="sd">                objects representing the chat history.</span>
<span class="sd">            context (:py:class:`ChatContext &lt;mlflow.types.agent.ChatContext&gt;`):</span>
<span class="sd">                A :py:class:`ChatContext &lt;mlflow.types.agent.ChatContext&gt;` object</span>
<span class="sd">                containing conversation_id and user_id. **Optional** Defaults to None.</span>
<span class="sd">            custom_inputs (Dict[str, Any]):</span>
<span class="sd">                An optional param to provide arbitrary additional inputs</span>
<span class="sd">                to the model. The dictionary values must be JSON-serializable. **Optional**</span>
<span class="sd">                Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A :py:class:`ChatAgentResponse &lt;mlflow.types.agent.ChatAgentResponse&gt;` object containing</span>
<span class="sd">            the model&#39;s response, as well as other metadata.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

    <span class="c1"># nb: We use `messages` instead of `model_input` so that the trace generated by default is</span>
    <span class="c1"># compatible with mlflow evaluate. We also want `custom_inputs` to be a top level key for</span>
    <span class="c1"># ease of use.</span>
<div class="viewcode-block" id="ChatAgent.predict_stream"><a class="viewcode-back" href="../../../python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent.predict_stream">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ChatAgentMessage</span><span class="p">],</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ChatContext</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">ChatAgentChunk</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a ChatAgent input, returns a generator containing streaming ChatAgent output chunks.</span>
<span class="sd">        In addition to calling ``predict_stream`` with an input matching the type hints, you can</span>
<span class="sd">        also pass a single input dict that matches the</span>
<span class="sd">        :py:class:`ChatAgentRequest &lt;mlflow.types.agent.ChatAgentRequest&gt;`</span>
<span class="sd">        schema for ease of testing.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            chat_agent = ChatAgent()</span>
<span class="sd">            for event in chat_agent.predict_stream(</span>
<span class="sd">                {</span>
<span class="sd">                    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 10 + 10?&quot;}],</span>
<span class="sd">                    &quot;context&quot;: {&quot;conversation_id&quot;: &quot;123&quot;, &quot;user_id&quot;: &quot;456&quot;},</span>
<span class="sd">                }</span>
<span class="sd">            ):</span>
<span class="sd">                print(event)</span>

<span class="sd">        To support streaming the output of your agent, override this method in your subclass of</span>
<span class="sd">        ``ChatAgent``. When implementing ``predict_stream``, keep in mind the following</span>
<span class="sd">        requirements:</span>

<span class="sd">        - Ensure your implementation adheres to the ``predict_stream`` type signature. For example,</span>
<span class="sd">          streamed messages must be of the type</span>
<span class="sd">          :py:class:`ChatAgentChunk &lt;mlflow.types.agent.ChatAgentChunk&gt;`, where each chunk contains</span>
<span class="sd">          partial output from a single response message.</span>
<span class="sd">        - At most one chunk in a particular response can contain the ``custom_outputs`` key.</span>
<span class="sd">        - Chunks containing partial content of a single response message must have the same ``id``.</span>
<span class="sd">          The content field of the message and usage stats of the</span>
<span class="sd">          :py:class:`ChatAgentChunk &lt;mlflow.types.agent.ChatAgentChunk&gt;` should be aggregated by</span>
<span class="sd">          the consuming client. See the example below.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {&quot;delta&quot;: {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Born&quot;, &quot;id&quot;: &quot;123&quot;}}</span>
<span class="sd">            {&quot;delta&quot;: {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; in&quot;, &quot;id&quot;: &quot;123&quot;}}</span>
<span class="sd">            {&quot;delta&quot;: {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; data&quot;, &quot;id&quot;: &quot;123&quot;}}</span>


<span class="sd">        Args:</span>
<span class="sd">            messages (List[:py:class:`ChatAgentMessage &lt;mlflow.types.agent.ChatAgentMessage&gt;`]):</span>
<span class="sd">                A list of :py:class:`ChatAgentMessage &lt;mlflow.types.agent.ChatAgentMessage&gt;`</span>
<span class="sd">                objects representing the chat history.</span>
<span class="sd">            context (:py:class:`ChatContext &lt;mlflow.types.agent.ChatContext&gt;`):</span>
<span class="sd">                A :py:class:`ChatContext &lt;mlflow.types.agent.ChatContext&gt;` object</span>
<span class="sd">                containing conversation_id and user_id. **Optional** Defaults to None.</span>
<span class="sd">            custom_inputs (Dict[str, Any]):</span>
<span class="sd">                An optional param to provide arbitrary additional inputs</span>
<span class="sd">                to the model. The dictionary values must be JSON-serializable. **Optional**</span>
<span class="sd">                Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A generator over :py:class:`ChatAgentChunk &lt;mlflow.types.agent.ChatAgentChunk&gt;`</span>
<span class="sd">            objects containing the model&#39;s response(s), as well as other metadata.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Streaming implementation not provided. Please override the &quot;</span>
            <span class="s2">&quot;`predict_stream` method on your model to generate streaming predictions&quot;</span>
        <span class="p">)</span></div></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_save_model_with_class_artifacts_params</span><span class="p">(</span>  <span class="c1"># noqa: D417</span>
    <span class="n">path</span><span class="p">,</span>
    <span class="n">python_model</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">artifacts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlflow_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">streamable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">model_code_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">infer_code_paths</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        path: The path to which to save the Python model.</span>
<span class="sd">        python_model: An instance of a subclass of :class:`~PythonModel`. ``python_model``</span>
<span class="sd">            defines how the model loads artifacts and how it performs inference.</span>
<span class="sd">        artifacts: A dictionary containing ``&lt;name, artifact_uri&gt;`` entries. Remote artifact URIs</span>
<span class="sd">            are resolved to absolute filesystem paths, producing a dictionary of</span>
<span class="sd">            ``&lt;name, absolute_path&gt;`` entries, (e.g. {&quot;file&quot;: &quot;absolute_path&quot;}).</span>
<span class="sd">            ``python_model`` can reference these resolved entries as the ``artifacts`` property</span>
<span class="sd">            of the ``context`` attribute. If ``&lt;artifact_name, &#39;hf:/repo_id&#39;&gt;``(e.g.</span>
<span class="sd">            {&quot;bert-tiny-model&quot;: &quot;hf:/prajjwal1/bert-tiny&quot;}) is provided, then the model can be</span>
<span class="sd">            fetched from huggingface hub using repo_id `prajjwal1/bert-tiny` directly. If ``None``,</span>
<span class="sd">            no artifacts are added to the model.</span>
<span class="sd">        conda_env: Either a dictionary representation of a Conda environment or the path to a Conda</span>
<span class="sd">            environment yaml file. If provided, this decsribes the environment this model should be</span>
<span class="sd">            run in. At minimum, it should specify the dependencies contained in</span>
<span class="sd">            :func:`get_default_conda_env()`. If ``None``, the default</span>
<span class="sd">            :func:`get_default_conda_env()` environment is added to the model.</span>
<span class="sd">        code_paths: A list of local filesystem paths to Python file dependencies (or directories</span>
<span class="sd">            containing file dependencies). These files are *prepended* to the system path before the</span>
<span class="sd">            model is loaded.</span>
<span class="sd">        mlflow_model: The model to which to add the ``mlflow.pyfunc`` flavor.</span>
<span class="sd">        model_config: The model configuration for the flavor. Model configuration is available</span>
<span class="sd">            during model loading time.</span>

<span class="sd">            .. Note:: Experimental: This parameter may change or be removed in a future release</span>
<span class="sd">                without warning.</span>

<span class="sd">        model_code_path: The path to the code that is being logged as a PyFunc model. Can be used</span>
<span class="sd">            to load python_model when python_model is None.</span>

<span class="sd">            .. Note:: Experimental: This parameter may change or be removed in a future release</span>
<span class="sd">                without warning.</span>

<span class="sd">        streamable: A boolean value indicating if the model supports streaming prediction,</span>
<span class="sd">                    If None, MLflow will try to inspect if the model supports streaming</span>
<span class="sd">                    by checking if `predict_stream` method exists. Default None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mlflow_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>

    <span class="n">custom_model_config_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">CONFIG_KEY_CLOUDPICKLE_VERSION</span><span class="p">:</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">python_model</span><span class="p">):</span>
        <span class="n">python_model</span> <span class="o">=</span> <span class="n">_FunctionPythonModel</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">python_model</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="n">saved_python_model_subpath</span> <span class="o">=</span> <span class="n">_SAVED_PYTHON_MODEL_SUBPATH</span>

    <span class="c1"># If model_code_path is defined, we load the model into python_model, but we don&#39;t want to</span>
    <span class="c1"># pickle/save the python_model since the module won&#39;t be able to be imported.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">model_code_path</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">saved_python_model_subpath</span><span class="p">),</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
                <span class="n">cloudpickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">python_model</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Failed to serialize Python model. Please save the model into a python file &quot;</span>
                <span class="s2">&quot;and use code-based logging method instead. See&quot;</span>
                <span class="s2">&quot;https://mlflow.org/docs/latest/models.html#models-from-code for more information.&quot;</span>
            <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>

        <span class="n">custom_model_config_kwargs</span><span class="p">[</span><span class="n">CONFIG_KEY_PYTHON_MODEL</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_python_model_subpath</span>

    <span class="k">if</span> <span class="n">artifacts</span><span class="p">:</span>
        <span class="n">saved_artifacts_config</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">with</span> <span class="n">TempDir</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmp_artifacts_dir</span><span class="p">:</span>
            <span class="n">saved_artifacts_dir_subpath</span> <span class="o">=</span> <span class="s2">&quot;artifacts&quot;</span>
            <span class="n">hf_prefix</span> <span class="o">=</span> <span class="s2">&quot;hf:/&quot;</span>
            <span class="k">for</span> <span class="n">artifact_name</span><span class="p">,</span> <span class="n">artifact_uri</span> <span class="ow">in</span> <span class="n">artifacts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">artifact_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">hf_prefix</span><span class="p">):</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">snapshot_download</span>
                    <span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                            <span class="s2">&quot;Failed to import huggingface_hub. Please install huggingface_hub &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;to log the model with artifact_uri </span><span class="si">{</span><span class="n">artifact_uri</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>

                    <span class="n">repo_id</span> <span class="o">=</span> <span class="n">artifact_uri</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">hf_prefix</span><span class="p">)</span> <span class="p">:]</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">snapshot_location</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span>
                            <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
                            <span class="n">local_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                                <span class="n">path</span><span class="p">,</span> <span class="n">saved_artifacts_dir_subpath</span><span class="p">,</span> <span class="n">artifact_name</span>
                            <span class="p">),</span>
                            <span class="n">local_dir_use_symlinks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                            <span class="s2">&quot;Failed to download snapshot from Hugging Face Hub with artifact_uri: &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">artifact_uri</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="n">saved_artifact_subpath</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">Path</span><span class="p">(</span><span class="n">snapshot_location</span><span class="p">)</span><span class="o">.</span><span class="n">relative_to</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">path</span><span class="p">)))</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tmp_artifact_path</span> <span class="o">=</span> <span class="n">_download_artifact_from_uri</span><span class="p">(</span>
                        <span class="n">artifact_uri</span><span class="o">=</span><span class="n">artifact_uri</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">tmp_artifacts_dir</span><span class="o">.</span><span class="n">path</span><span class="p">()</span>
                    <span class="p">)</span>

                    <span class="n">relative_path</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">Path</span><span class="p">(</span><span class="n">tmp_artifact_path</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">relative_to</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">tmp_artifacts_dir</span><span class="o">.</span><span class="n">path</span><span class="p">()))</span>
                        <span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
                    <span class="p">)</span>

                    <span class="n">saved_artifact_subpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">saved_artifacts_dir_subpath</span><span class="p">,</span> <span class="n">relative_path</span>
                    <span class="p">)</span>

                <span class="n">saved_artifacts_config</span><span class="p">[</span><span class="n">artifact_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">CONFIG_KEY_ARTIFACT_RELATIVE_PATH</span><span class="p">:</span> <span class="n">saved_artifact_subpath</span><span class="p">,</span>
                    <span class="n">CONFIG_KEY_ARTIFACT_URI</span><span class="p">:</span> <span class="n">artifact_uri</span><span class="p">,</span>
                <span class="p">}</span>

            <span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">tmp_artifacts_dir</span><span class="o">.</span><span class="n">path</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">saved_artifacts_dir_subpath</span><span class="p">))</span>
        <span class="n">custom_model_config_kwargs</span><span class="p">[</span><span class="n">CONFIG_KEY_ARTIFACTS</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_artifacts_config</span>

    <span class="k">if</span> <span class="n">streamable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">streamable</span> <span class="o">=</span> <span class="n">python_model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">predict_stream</span> <span class="o">!=</span> <span class="n">PythonModel</span><span class="o">.</span><span class="n">predict_stream</span>

    <span class="k">if</span> <span class="n">model_code_path</span><span class="p">:</span>
        <span class="n">loader_module</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">code_model</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">elif</span> <span class="n">python_model</span><span class="p">:</span>
        <span class="n">loader_module</span> <span class="o">=</span> <span class="n">_get_pyfunc_loader_module</span><span class="p">(</span><span class="n">python_model</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="s2">&quot;Either `python_model` or `model_code_path` must be provided to save the model.&quot;</span><span class="p">,</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">add_to_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">mlflow_model</span><span class="p">,</span>
        <span class="n">loader_module</span><span class="o">=</span><span class="n">loader_module</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">python_env</span><span class="o">=</span><span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
        <span class="n">streamable</span><span class="o">=</span><span class="n">streamable</span><span class="p">,</span>
        <span class="n">model_code_path</span><span class="o">=</span><span class="n">model_code_path</span><span class="p">,</span>
        <span class="o">**</span><span class="n">custom_model_config_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="o">:=</span> <span class="n">get_total_file_size</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">model_size_bytes</span> <span class="o">=</span> <span class="n">size</span>
    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">))</span>

    <span class="n">saved_code_subpath</span> <span class="o">=</span> <span class="n">_validate_infer_and_copy_code_paths</span><span class="p">(</span>
        <span class="n">code_paths</span><span class="p">,</span>
        <span class="n">path</span><span class="p">,</span>
        <span class="n">infer_code_paths</span><span class="p">,</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">][</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">CODE</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_code_subpath</span>

    <span class="c1"># `mlflow_model.code` is updated, re-generate `MLmodel` file.</span>
    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">conda_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pip_requirements</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="n">get_default_pip_requirements</span><span class="p">()</span>
            <span class="n">extra_env_vars</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_get_databricks_serverless_env_vars</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">is_in_databricks_serverless_runtime</span><span class="p">()</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="c1"># To ensure `_load_pyfunc` can successfully load the model during the dependency</span>
            <span class="c1"># inference, `mlflow_model.save` must be called beforehand to save an MLmodel file.</span>
            <span class="n">inferred_reqs</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_pip_requirements</span><span class="p">(</span>
                <span class="n">path</span><span class="p">,</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span>
                <span class="n">fallback</span><span class="o">=</span><span class="n">default_reqs</span><span class="p">,</span>
                <span class="n">extra_env_vars</span><span class="o">=</span><span class="n">extra_env_vars</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">inferred_reqs</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">default_reqs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_pip_requirements</span><span class="p">(</span>
            <span class="n">default_reqs</span><span class="p">,</span>
            <span class="n">pip_requirements</span><span class="p">,</span>
            <span class="n">extra_pip_requirements</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_conda_env</span><span class="p">(</span><span class="n">conda_env</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">safe_dump</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Save `constraints.txt` if necessary</span>
    <span class="k">if</span> <span class="n">pip_constraints</span><span class="p">:</span>
        <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_constraints</span><span class="p">))</span>

    <span class="c1"># Save `requirements.txt`</span>
    <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_requirements</span><span class="p">))</span>

    <span class="n">_PythonEnv</span><span class="o">.</span><span class="n">current</span><span class="p">()</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_load_context_model_and_signature</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">):</span>
    <span class="n">pyfunc_config</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span>
        <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">FLAVOR_NAME</span>
    <span class="p">)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span><span class="o">.</span><span class="n">signature</span>

    <span class="k">if</span> <span class="n">MODEL_CODE_PATH</span> <span class="ow">in</span> <span class="n">pyfunc_config</span><span class="p">:</span>
        <span class="n">conf_model_code_path</span> <span class="o">=</span> <span class="n">pyfunc_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">MODEL_CODE_PATH</span><span class="p">)</span>
        <span class="n">model_code_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">conf_model_code_path</span><span class="p">))</span>
        <span class="n">python_model</span> <span class="o">=</span> <span class="n">_load_model_code_path</span><span class="p">(</span><span class="n">model_code_path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">python_model</span><span class="p">):</span>
            <span class="n">python_model</span> <span class="o">=</span> <span class="n">_FunctionPythonModel</span><span class="p">(</span><span class="n">python_model</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">python_model_cloudpickle_version</span> <span class="o">=</span> <span class="n">pyfunc_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">CONFIG_KEY_CLOUDPICKLE_VERSION</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">python_model_cloudpickle_version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The version of CloudPickle used to save the model could not be found in the &quot;</span>
                <span class="s2">&quot;MLmodel configuration&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">python_model_cloudpickle_version</span> <span class="o">!=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">__version__</span><span class="p">:</span>
            <span class="c1"># CloudPickle does not have a well-defined cross-version compatibility policy. Micro</span>
            <span class="c1"># version releases have been known to cause incompatibilities. Therefore, we match on</span>
            <span class="c1"># the full library version</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The version of CloudPickle that was used to save the model, `CloudPickle </span><span class="si">%s</span><span class="s2">`, &quot;</span>
                <span class="s2">&quot;differs from the version of CloudPickle that is currently running, `CloudPickle &quot;</span>
                <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">`, and may be incompatible&quot;</span><span class="p">,</span>
                <span class="n">python_model_cloudpickle_version</span><span class="p">,</span>
                <span class="n">cloudpickle</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">python_model_subpath</span> <span class="o">=</span> <span class="n">pyfunc_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">CONFIG_KEY_PYTHON_MODEL</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">python_model_subpath</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span><span class="s2">&quot;Python model path was not specified in the model configuration&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">python_model_subpath</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">python_model</span> <span class="o">=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="n">artifacts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">saved_artifact_name</span><span class="p">,</span> <span class="n">saved_artifact_info</span> <span class="ow">in</span> <span class="n">pyfunc_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="n">CONFIG_KEY_ARTIFACTS</span><span class="p">,</span> <span class="p">{}</span>
    <span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">artifacts</span><span class="p">[</span><span class="n">saved_artifact_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">model_path</span><span class="p">,</span> <span class="n">saved_artifact_info</span><span class="p">[</span><span class="n">CONFIG_KEY_ARTIFACT_RELATIVE_PATH</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="n">context</span> <span class="o">=</span> <span class="n">PythonModelContext</span><span class="p">(</span><span class="n">artifacts</span><span class="o">=</span><span class="n">artifacts</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>
    <span class="n">python_model</span><span class="o">.</span><span class="n">load_context</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">python_model</span><span class="p">,</span> <span class="n">signature</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_load_pyfunc</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">context</span><span class="p">,</span> <span class="n">python_model</span><span class="p">,</span> <span class="n">signature</span> <span class="o">=</span> <span class="n">_load_context_model_and_signature</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_PythonModelPyfuncWrapper</span><span class="p">(</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">python_model</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_first_string_column</span><span class="p">(</span><span class="n">pdf</span><span class="p">):</span>
    <span class="n">iter_string_columns</span> <span class="o">=</span> <span class="p">(</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">str</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="n">iter_string_columns</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_PythonModelPyfuncWrapper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper class that creates a predict function such that</span>
<span class="sd">    predict(model_input: pd.DataFrame) -&gt; model&#39;s output as pd.DataFrame (pandas DataFrame)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">python_model</span><span class="p">:</span> <span class="n">PythonModel</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">signature</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            python_model: An instance of a subclass of :class:`~PythonModel`.</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that</span>
<span class="sd">                     ``python_model`` may use when performing inference.</span>
<span class="sd">            signature: :class:`~ModelSignature` instance describing model input and output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">python_model</span> <span class="o">=</span> <span class="n">python_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_convert_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_input</span><span class="p">):</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

        <span class="n">hints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict_type_hints</span>
        <span class="c1"># we still need this for backwards compatibility</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_is_list_str</span><span class="p">(</span><span class="n">hints</span><span class="o">.</span><span class="n">input</span><span class="p">):</span>
                <span class="n">first_string_column</span> <span class="o">=</span> <span class="n">_get_first_string_column</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">first_string_column</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">MlflowException</span><span class="o">.</span><span class="n">invalid_parameter_value</span><span class="p">(</span>
                        <span class="s2">&quot;Expected model input to contain at least one string column&quot;</span>
                    <span class="p">)</span>
                <span class="k">return</span> <span class="n">model_input</span><span class="p">[</span><span class="n">first_string_column</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">_is_list_dict_str</span><span class="p">(</span><span class="n">hints</span><span class="o">.</span><span class="n">input</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                    <span class="ow">and</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">inputs</span><span class="p">))</span><span class="o">.</span><span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="n">first_string_column</span> <span class="o">:=</span> <span class="n">_get_first_string_column</span><span class="p">(</span><span class="n">model_input</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">model_input</span><span class="p">[[</span><span class="n">first_string_column</span><span class="p">]]</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">model_input</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;list&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">model_input</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hints</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">issubclass</span><span class="p">(</span><span class="n">hints</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="n">ChatCompletionRequest</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">hints</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="n">SplitChatMessagesRequest</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="c1"># If the type hint is a RAG dataclass, we hydrate it</span>
                <span class="c1"># If there are multiple rows, we should throw</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                        <span class="s2">&quot;Expected a single input for dataclass type hint, but got multiple rows&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># Since single input is expected, we take the first row</span>
                <span class="k">return</span> <span class="n">_hydrate_dataclass</span><span class="p">(</span><span class="n">hints</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="n">model_input</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">model_input</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            model_input: Model input data as one of dict, str, bool, bytes, float, int, str type.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model predictions as an iterator of chunks. The chunks in the iterator must be type of</span>
<span class="sd">            dict or string. Chunk dict fields are determined by the model implementation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;params&quot;</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_log_warning_if_params_not_in_predict_signature</span><span class="p">(</span><span class="n">_logger</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_is_context_in_predict_function_signature</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="n">model_input</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="n">model_input</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            model_input: LLM Model single input.</span>
<span class="sd">            params: Additional parameters to pass to the model for inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Streaming predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict_stream</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;params&quot;</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_log_warning_if_params_not_in_predict_signature</span><span class="p">(</span><span class="n">_logger</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_is_context_in_predict_function_signature</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict_stream</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="n">model_input</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">python_model</span><span class="o">.</span><span class="n">predict_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="n">model_input</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_pyfunc_loader_module</span><span class="p">(</span><span class="n">python_model</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">python_model</span><span class="p">,</span> <span class="n">ChatModel</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">chat_model</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">python_model</span><span class="p">,</span> <span class="n">ChatAgent</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">chat_agent</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">return</span> <span class="vm">__name__</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ModelFromDeploymentEndpoint</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A PythonModel wrapper for invoking an MLflow Deployments endpoint.</span>
<span class="sd">    This class is particularly used for running evaluation against an MLflow Deployments endpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">endpoint</span> <span class="o">=</span> <span class="n">endpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run prediction on the input data.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: A :class:`~PythonModelContext` instance containing artifacts that the model</span>
<span class="sd">                can use to perform inference.</span>
<span class="sd">            model_input: The input data for prediction, either of the following:</span>
<span class="sd">                - Pandas DataFrame: If the default evaluator is used, input is a DF</span>
<span class="sd">                    that contains the multiple request payloads in a single column.</span>
<span class="sd">                - A dictionary: If the model_type is &quot;databricks-agents&quot; and the</span>
<span class="sd">                    Databricks RAG evaluator is used, this PythonModel can be invoked</span>
<span class="sd">                    with a single dict corresponding to the ChatCompletionsRequest schema.</span>
<span class="sd">                - A list of dictionaries: Currently we don&#39;t have any evaluator that</span>
<span class="sd">                    gives this input format, but we keep this for future use cases and</span>
<span class="sd">                    compatibility with normal pyfunc models.</span>

<span class="sd">        Return:</span>
<span class="sd">            The prediction result. The return type will be consistent with the model input type,</span>
<span class="sd">            e.g., if the input is a Pandas DataFrame, the return will be a Pandas Series.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_single</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">model_input</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict_single</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">model_input</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The number of input columns must be 1, but got </span><span class="si">{</span><span class="n">model_input</span><span class="o">.</span><span class="n">columns</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Multi-column input is not supported for evaluating an MLflow Deployments &quot;</span>
                    <span class="s2">&quot;endpoint. Please include the input text or payload in a single column.&quot;</span><span class="p">,</span>
                    <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">input_column</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict_single</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">model_input</span><span class="p">[</span><span class="n">input_column</span><span class="p">]]</span>
            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid input data type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span><span class="si">}</span><span class="s2">. The input data must be either &quot;</span>
                <span class="s2">&quot;a Pandas DataFrame, a dictionary, or a list of dictionaries containing the &quot;</span>
                <span class="s2">&quot;request payloads for evaluating an MLflow Deployments endpoint.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_predict_single</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Send a single prediction request to the MLflow Deployments endpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: The single input data for prediction. If the input data is a string, we will</span>
<span class="sd">                construct the request payload from it. If the input data is a dictionary, we</span>
<span class="sd">                will directly use it as the request payload.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The prediction result from the MLflow Deployments endpoint as a dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.metrics.genai.model_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">call_deployments_api</span><span class="p">,</span> <span class="n">get_endpoint_type</span>

        <span class="n">endpoint_type</span> <span class="o">=</span> <span class="n">get_endpoint_type</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;endpoints:/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">endpoint</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># If the input payload is string, MLflow needs to construct the JSON</span>
            <span class="c1"># payload based on the endpoint type. If the endpoint type is not</span>
            <span class="c1"># set on the endpoint, we will default to chat format.</span>
            <span class="n">endpoint_type</span> <span class="o">=</span> <span class="n">endpoint_type</span> <span class="ow">or</span> <span class="s2">&quot;llm/v1/chat&quot;</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">call_deployments_api</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">endpoint</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">endpoint_type</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># If the input is dictionary, we assume the input is already in the</span>
            <span class="c1"># compatible format for the endpoint.</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">call_deployments_api</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">endpoint</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">endpoint_type</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid input data type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">. The feature column of the evaluation &quot;</span>
                <span class="s2">&quot;dataset must contain only strings or dictionaries containing the request &quot;</span>
                <span class="s2">&quot;payload for evaluating an MLflow Deployments endpoint.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../',
      VERSION:'2.20.3',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>